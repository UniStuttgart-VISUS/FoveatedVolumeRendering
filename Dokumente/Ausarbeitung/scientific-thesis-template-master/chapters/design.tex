% Entwurf: Projekt, Raytracer, (Auswahl des) Eyetracker, Vorüberlegungen zur Umsetzung / Integration, Grundidee zur Umsetzung
% Entwurfsziele an die Umsetzungs
\chapter{Entwurf}\label{chap::design}
Die Implementierung dieser Arbeit beruht auf einem Visual Studio Projekt zum Volumen Rendering von Valentin Bruder.
Der erste Abschnitt des Entwurfskapitels beinhaltet den Ausgangspunkt der Implementierung dieser Arbeit in Form einer Beschreibung des ursprünglichen Projekts.
Dies umfasst die allgemeine Architektur der Anwendung und die Umsetzung des Volumenrenderings durch eine Implementierung eines Raycasters als OpenCL Kernel.
Der zweite Abschnitt des Kapitels umfasst Überlegungen für die Erweiterung des Projektes, bezüglich des Ausgangspunktes wie er im ersten Abschnitt beschrieben wurde und dem Ziel dieser Arbeit, sowie die daraus entstandenen Arbeitspakete und die Ansätze für ihre Integration in das bestehende Projekt.

\section{Projekt}\label{sec::proj}
% QT
Das Visual Studio Projekt, welches als Ausgangspunkt dient, ist eine auf QT basierende Anwendung.
QT ist ein vollständiges Cross-Platform Softaware Development Framework, welches in c++ entwickelt wurde, und ermöglicht die einfache Erstellung von Anwendungen mit Benutzeroberflächen und bietet außerdem eine Vielzahl von Bibliotheken, die für eine leichtere und schnellere Entwicklung von Programmen genutzt werden können.

% Struktur von QT
Die Hauptelemente für die QT Benutzeroberfläche sind QT Widgets.
Widgets können Daten darstellen und Nutzereingaben erkennen.
Außerdem stellt ein Widget selbst ein Container für weitere Widgets dar, welche in diesem gruppiert werden.
Innerhalb eines Widgets können Elemente platziert werden, welche Informationen, mögliche Operationen oder Nutzereingaben repräsentieren.
Für die bequeme Erstellung der grafischen Oberfläche bietet QT die Anwendung QT Designer.
Der QT Designer ermöglicht es, Widgets und andere Bausteine der grafischen Oberfläche per Drag und Drop anzuordnen.
Die durch den QT Designer definierte Oberfläche kann abgespeichert werden und wird von QT verwendet, um eine c++ Header File zu erstellen.
Dies ermöglicht es die verschiedenen Elemente der grafischen Oberfläche an die Logik der Anwendung zu binden.
\todo{Quelle zu QT einfügen.}

% Struktur des Projekts (verschiedene Widgets)
Das Projekt ist dementsprechend in c++ geschrieben und die grafische Oberfläche wurde mit Hilfe des QT Designers erstellt.
Die Oberfläche selbst hat eine Menüleiste, die es unter Anderem ermöglicht, Volumendaten oder Transferfunktionen zu laden oder auch aktive Transferfunktionen zu speichern sowie das Erstellen eines Screenshots des zuletzt berechneten Bildes.
Den Großteil der grafischen Oberfläche wird durch das Volumenrenderwidget ausgefüllt.
Das Volumenrenderwidget ist ein QT OpenGL Widget und kann für das Darstellen von OpenGL Grafiken verwendet werden.
Die berechneten Grafiken werden mit Hilfe des Volumenrenderwidgets dargestellt.
Neben dem Volumenrenderwidget gibt es noch ein Widget, welches in drei weitere Widgets unterteilt ist, mit denen Parameter für das Volumenrendering gesetzt werden können.
Das erste von ihnen ermöglicht das variieren der Abtastrate im Bildraum, also die Anzahl der Strahlen, die ausgesendet werden, sowie das Setzen der allgemeinen Abtastrate der ausgesendeten Strahlen.
Außerdem können hier weitere Rendering Parameter festgelegt werden, wie die Hintergrundfarbe oder ob Voxel beim Abtasten interpoliert werden.
Das zweite Widget ist ein Farbenrad, welches für die einfache Auswahl der Farben einzelner Kontrollpunkte der Transferfunktion verwendet werden kann.
Das dritte Widget ermöglicht schließlich das Setzen von Kontrollpunkten der Transferfunktion innerhalb eines Diagramms.
Die x-Richtung gibt die Dichte, auf die sich ein Kontrollpunkt bezieht an.
Die y-Richtung gibt seinen Opazitätswert an.
Die Werte zwischen zwei Kontrollpunkten werden entweder linear oder quadratisch interpoliert.
Daher gibt es immer mindestens einen Kontrollpunkt für den Dichtewert null und einen Kontrollpunkt für den Dichtewert eins.
\todo{Bitte auf Richtigkeit von diesem Teil prüfen, wie: Interpolation von Voxel und Einstellen der Transferfunkton.}

% Speziell VolumeRenderWidget: OpenCL, OpenGL Host Code
Das Volumenrenderwidget ist ein OpenGL Widget und für die Darstellung der berechneten Bilder zuständig.
Das QT Framework erlaubt es, das Widget in c++ Code mit Logik zu verknüpfen.
Dafür existiert in dem Projekt eine Klasse VolumeRenderWidget, welche von QOpenGLWidget erbt.
Die grundsätzliche Funktionalität zum Rendern in diesem Widget wird durch die Methode \texttt{paintGL()} ausgeführt.
Innerhalb dieser Methode wird der Code geschrieben, der für die Darstellung des Bildes nötig ist.
Das Darstellen auf dem Bildschirm beziehungsweise in dem Widget wird durch OpenGL realisiert.
OpenGL zeichnet dabei aber lediglich eine durch einen OpenCL Kernel zuvor generierte Textur auf ein Fullscreen Quad.
Das Management von OpenCL wird hier durch ein Objekt der Klasse \texttt{volumerendercl} geregelt.
Innerhalb der \texttt{paintGL()} Methode wird eine Methode dieses Objekts zum Starten des OpenCL Kernels für den Raycast des Volumenrenderings aufgerufen.
Das \texttt{volumerendercl} Objekt regelt die Handhabung der verschiedenen Parameter für den Kernel und die Unterteilung der übergebenen Anzahl an Strahlen in x- und y-Richtung, welche der Anzahl zu startenden Work-Items entspricht, in Work-Groups.
Außerdem startet es den Kernel, synchronisiert einen gemeinsamen Stopp und speichert die benötigte Zeit der letzten Ausführung des Kernels.
Die berechneten Werte der einzelnen Work-Items können bei der Ausführung des Kernels direkt in die OpenGL Textur geschrieben werden.
Daher kann nach der Ausführung der aufgerufenen Methode des \texttt{volumerendercl} Objekts die Textur direkt gezeichnet werden.
Mit Hilfe von QT Funktionen und der Information über die Ausführungszeit des Kernels werden anschließend noch ein paar Overlays gezeichnet.
Unter Anderem eine Anzeige der ungefähren möglichen Anzahl an Bildern pro Sekunde der letzten Ausführungen, um die Ausführungsdauer des Kernels abschätzen zu können.

% Raycaster: Raycastkernel
\subsection*{Raycaster}\label{ss::rc}
% Raycaster: wichtige Parameter: in, out; Ungefährerer Aufbau: Sampling Loop durch das Volumen;
Der eigentliche Raycast passiert in einem OpenCL Kernel.
Die OpenCL Objekte werden von dem \texttt{volumerendercl} Objekt gehandhabt, dessen Methoden innerhalb der \texttt{paintGL()} Methode aufgerufen werden.
Das \texttt{volumerendercl} Objekt regelt auch die Übergabe der Parameter an den Raycast Kernel.
Dies sind Parameter wie Volumendaten, Transferfunktionswerte und Strahlabtastrate zum lesen, sowie eine 2D-Textur zum schreiben für die berechneten Farbwerte der einzelnen Work-Items.
Jedes Work-Item besitzt eine 2D-ID, die einer Position in der Ausgabetextur zugewiesen bekommt.
Ein Work-Item ist für das Abtasten eines Strahls verantwortlich.
Ein Strahl hat als Ursprung die Position der Kamera und entsprechend seiner ID, beziehungsweise Texturkoordinaten, wird seine Richtung bestimmt.
Abhängig von der Abtastrate des Strahls, berechnet sich die Schrittgröße für das Abtasten. 
Ausgehend von dem Schnittpunkt des Strahls mit der Bildebene wird nun in einer Schleife der Strahl schrittweise abgetastet.
Dabei wird für jeden Schritt die aktuelle Position bestimmt, welche normiert und dann dafür genutzt wird, um in dem 3D-Volumen Objekt den Dichtewert für diese Position zu bestimmen.
Mit dem Dichtewert und den Daten der Transferfunktion wird anschließend ein Farbwert berechnet.
Dieser Farbwert wird mit den bisherigen gesammelten Farbwerten des Strahls verrechnet, so dass am Ende der Abtastschleife ein einziger Farbwert für den Strahl existiert.
Der Farbwert wird zum Schluss an die entsprechende Texturkoordinate in der Ausgabetextur gespeichert.
\todo{Bild für das Abtasten des Strahls einfügen. Entweder hier oder bei Basics mit Volumenrendering und dann dorthin verweisen.}

%  Raycaster: Spezielle Eigenschaften, die aktiviert und deaktiviert werden können: ESS, Interpolation, AO
Der Raycast Kernel hat außer der grundlegenden Raycast Funktion noch weitere Eigenschaften, die die Performanz der Ausführung und die Qualität des Bildes verbessern.
So kann \emph{Empty Space Skipping} aktiviert werden, um größere Bereiche mit rein transparenten Voxeln zu überspringen.
Dies wird mit Hilfe eines zuvor gröber Berechneten Volumen ermöglicht.
Da das Volumen nur eine begrenzte Auflösung hat aber an einer beliebigen Position ein Wert aus dieser 3D-Textur abgerufen werden kann, muss angegeben werden, wie dieser Wert abhängig von den umliegenden Voxel bestimmt wird.
Daher kann hier gewählt werden, dass beim Auslesen des Dichtewerts an einer bestimmten Position des Volumens, dieser interpoliert wird.
Außerdem kann eine Orthografische Sicht des Volumens aktiviert werden, indem die Strahlen parallel ausgesendet werden und für solide Oberflächen gibt es die Möglichkeit, den Effekt der \emph{Ambient Occlusion} darzustellen.
\todo{Beschreibung der Funktionsweise des Raycasters für das Volumenrendering in dem Projekt.}

\section{Arbeitspakete und Integration}\label{sec::workpacks}
Ausgehend von der in Abschnitt \ref{sec::proj} beschriebenen Ausgangslage des Projekts, wurden einige Vorüberlegungen und Arbeitspakete erstellt, welche das Ziel hatten, das Projekt so zu erweitern, dass die Aspekte des wahrnehmungsorientierten Volumenrendering veranschaulicht und umgesetzt werden können.
Im folgenden werden die für dieses Ziel entstandenen Vorüberlegungen und daraus erstellte Arbeitspakete aufgeführt und die dazugehörigen Ansätze zur Integration in das bestehende Projekt skizziert.
Genauere Angaben zur Implementierung bestimmter Arbeitspakete werden im Kapitel \ref{chap::impl} vorgestellt.

\subsection{Einarbeitung in das Projekt}
Das erste Arbeitspaket, welches nicht zu vernachlässigen ist, war die Einarbeitung in das Projekt beziehungsweise in die bestehende Implementierung.
Dies erforderte das Einarbeiten in einige Grundlagen der c++ Programmierung sowie das Einarbeiten in den grundlegenden Umgang mit dem QT Framework.
Da das Projekt ein Visual Studio Projekt ist und Programmierschnittstellen wie OpenCL oder auch QT verwendet, war eine kleine Einarbeitung in das richtige Verlinken der Bibliotheken mit dem Projekt auch Teil von diesem Arbeitspaket.

\subsection{Simulieren der Blickposition}
Um die Eigenschaften des visuellen Wahrnehmungssystems des Menschen auszunutzen, dass die Genauigkeit des Auges außerhalb des zentralen Bereichs stark abnimmt, ist es notwendig, die Blickposition beziehungsweise den fokussierten Punkt auf dem Bildschirm zu kennen.
Ein Eyetracker kann dies messen und die Daten der Anwendung zur Verfügung stellen.
Da die Einbindung eines Eyetrackers für die ersten Arbeitsschritte, wie das Implementieren erster Versuche, den Raycast Kernel wahrnehmungsorientiert umzuschreiben, nicht notwendig ist, sondern zum Teil auch erschwert, kann der Blickpunkt vorerst sehr gut mit der Maus simuliert werden.
QT Widgets können auf Maus und Tastatureingaben reagieren.
Daher war das erste Arbeitspaket das Erkennen von Mausbewegungen innerhalb des Volumerenderwidgets und das Abspeichern der letzten erkannten Mausposition in einer globalen Variable.
Zusätzlich musste diese dem OpenCL Kernel, der den Raycast ausführt, zur Verfügung gestellt werden.
Daher wurde die Mausposition dem OpenCL Kernel als Parameter übergeben.

\subsection{Reduzierung der Strahlabtastrate im fovealen Bereich}
Da die Mausposition dem Kernel nun zur Verfügung steht, können erste Implementierungsversuche für einen wahrnehmungsorientierten Raycast unternommen werden.
Das ursprüngliche Projekt stellt zwei Parameter zur Verfügung, welche auf zwei verschiedene Arten die Ausführungszeit des Kernels beeinflussen.
Die Abtastrate der jeweiligen Strahlen und die Anzahl der Strahlen in x- und y-Richtung.
Da das Verändern der Anzahl an Strahlen in x- und y- Richtung das gesamte Bild betrifft und dies nicht einfach abhängig von dem Abstand zur Mausposition verändert werden kann, ist die Anpassung der Abtastrate einzelner Strahlen für den Anfang einfacher zu gestalten.
Aus diesem Grund heraus entstand das nächste Arbeitspaket.
Dieses umfasst die Verwendung der an den Raycast Kernel übergebenen Mausposition, um die Abtastrate der jeweiligen Strahlen, abhängig von der Distanz der Bildposition des Strahls zu der Position des Mauszeigers, zu reduzieren.
Die Anpassung des Kernels hat zur Folge, dass für jeden Strahl abhängig seiner Distanz zum Mauszeiger eine eigene Abtastrate berechnet wird.
Aufgrund dessen, dass wie im Abschnitt \ref{ss::rc} die Work-Items den Texel zugeordnet sind und die Work-Groups quadratisch angeordnet sind, sollte dies bewirken, dass die Work-Items innerhalb der selben Work-Group eine ähnliche Abtastrate für ihren Strahl berechnen und die gesamte Work-Group früher terminieren kann.
Da der Kernel erst beendet wird, wenn alle Work-Groups ihre Arbeit abgeschlossen haben, bewirkt eine schnellere Ausführung einzelner Work-Groups eine insgesamt schnellere Ausführung des Kernels.
Dementsprechend bewirkt dies auch eine bessere Performanz beim Berechnen des Bildes.
Die Mausposition wurde in Bildkoordinaten, bezüglich des Volumerenderwidgets dem Kernel übergeben.
Da die Anzahl der gestarteten Work-Items den Ausmaßen des Volumerenderwidgets entspricht, entspricht auch die ID eines Work-Items einer Bildkoordinate.
Daher konnte die Entfernung der Work-Items zum Mauszeiger einfach berechnet werden und die Abtastrate abhängig von dieser Distanz angepasst.
Erste Tests der Implementierung haben eine Erhöhung der im Volumerenderwidget durchschnittlichen angezeigten Bilder pro Sekunde ergeben.
Die Verminderung der Abtastrate machte sich nur bei dünnen Volumen visuell bemerkbar, da hier die einige Strahlen einen Teil des Volumen teilweise gar nicht abgetastet haben.

\subsection{Reduzierung der Strahldichte im fovealen Bereich}\label{ss::MDC}
Trotz der Reduzierung der Abtastrate der Strahlen, wird weiterhin die gleiche Anzahl an Work-Items gestartet.
Dies ermöglicht eine weitere Möglichkeit, die Ausführungszeit des Kernels zu reduzieren, indem die Anzahl der Strahlen und somit auch die Auflösung des berechneten Bildes variiert wird.
Weniger zu berechnenden Strahlen bedeutet hier auch weniger benötigte Work-Items und Work-Groups, die ausgeführt werden müssen.
Weniger Work-Groups bedeutet dann auch eine geringere Ausführungszeit des Raycast Kernels.

Die erste Überlegung diesbezüglich war es, eine Art virtuelle Linse vor die Bildebene zu setzen, die die Strahlen so auf der Bildebene verteilt, dass an der Mausposition eine höhere Strahldichte existiert und diese mit größerem Abstand zur Mausposition abnimmt.
So könnte bei einer geringeren Anzahl an Strahlen, an der Mausposition, die den Blickpunkt simuliert, trotzdem die maximale Auflösung erreicht werden.
Die Bildpunkte die dann nicht direkt von einem Strahl abgedeckt werden, müssten interpoliert werden.
Dieser Ansatz wurde aber verworfen, da die Implementierung der virtuellen Linse und der anschließenden Interpolation sich als zu aufwändig erwies und es deutlich einfacher umzusetzende Alternativen gibt.

Eine Alternative ist es, zwei Mal den Raycast Kernel hintereinander, mit jeweils unterschiedlichen Auflösungen, also einer unterschiedlichen Anzahl an Work-Items, zu starten.
Dies wurde das nächste Arbeitspaket: Das Berechnen des Bildes in zwei verschiedenen Auflösungen und die anschließende Zusammenfügung beider Bilder zu einem.
Für die Integration wurde wie in dem Arbeitspaket gefordert, das Bild in zwei verschiedenen Auflösungen nacheinander berechnet.
Dafür konnte die bisherigen Funktionen des \texttt{volumerendercl} Objektes weiter verwendet werden.
Es wurden aber zwei unterschiedliche OpenGL Texturen für die Ausgabe des Kernels verwendet.
Zuerst mit nur einem viertel der Auflösung und anschließend mit der normalen Auflösung.
Innerhalb eines festgelegten Quadrats um den Mauszeiger herum ist die Auflösung des Bildes normal und außerhalb davon hat diese nur ein viertel der normalen Auflösung.
Bei der ersten Berechnung wird der Teil innerhalb von diesem Quadrat nicht berechnet und entsprechend wird bei der zweiten Berechnung der Teil des Bildes außerhalb des Quadrats nicht berechnet.
Dadurch werden so wenige Bildpunkte wie möglich doppelt berechnet.
Da das erste Bild nur mit einem viertel der Auflösung berechnet wurde und die ID der Work-Items nicht mehr mit den Bildkoordinaten übereinstimmte, wurde die Mausposition und die Ausmaße des Quadrates sowohl bei der Berechnung des Bildes, als auch bei der Zusammenfügung beider Bilder normalisiert.

\subsection{Indize-Mapping}\label{ss::DDC}
Das Ziel eines weiteren Ansatzes war es, statt nur zwei verschiedene Auflösungen zu verwenden, welche in Form eines Quadrats übereinander gelegt werden, nun drei verschiedene Bereiche des Bildes mit je unterschiedlichen Auflösungen zu berechnen.
Ein äußerer Bereich und zwei innere Bereiche, die die Form von Ellipsen haben.
Die drei Bereiche ergeben dann das gesamte Bild.

Die erste Überlegung für diesen Ansatz war es, wie bei der Umsetzung des vorherigen Arbeitspaketes, auch hier drei Mal eine Berechnung für das gesamte Bild zu starten, aber jeweils die Teile, die außerhalb des Bereichs der aktuellen Berechnung liegen, zu discarden also diese nicht zu berechnen.
Ein früher erster Test von diesem Ansatz hat gezeigt, dass die Performanz deutlich schlechter war, als bei der vorherigen Methode.
Trotzdem zeigten die aufaddierten Ausführungszeiten der Kernels gute Werte.
Ein mögliches Problem dafür ist, dass der Overhead für das Starten der Kernels bei drei verschiedenen Ausführungen doch einen Einfluss auf die Performanz nimmt.

Da bis zu drei Ausführungen des Raycast Kernels hintereinander nicht die gewünschte Performanz erbrachten, wurde dies verworfen und eine weitere Möglichkeit, das oben genannte Ziel zu erreichen, überlegt.
In der zweiten Überlegung ging es nun darum, mit nur einem Raycast Kernel Aufruf eine Berechnung für das ganze Bild zu starten.
Jedoch werden in den äußeren zwei Bereichen nur in x- und y-Richtung nur jede dritte beziehungsweise jede zweite Berechnung eines Work-Items ausgeführt.
Die anderen Work-Items werden entsprechend früh discarded.
Ein zweiter Kernel soll die fehlenden Bildpunkte durch bipolare Interpolation der umliegenden Bildpunkte ausfüllen, so dass ein einzelnes Bild mit normaler Auflösung entsteht.
Hier hat ein erster Test, bei dem lediglich ein Raycast Kernel aufgerufen wurde aber in x- und y-Richtung jedes zweite Work-Item discarded wurde gezeigt, dass dies keinerlei Performanz bringt.
Dies liegt daran, dass durch die Anordnung der Work-Groups keine Work-Group existierte, die nur aus Work-Items bestand, die discarded wurden.
Da vermutlich eine Work-Group auf einen Thread-Block abgebildet wird und ein Thread-Block so lange ausführt, bis alle seine Thread terminiert sind, wurde bei diesem Ansatz zwar nur ein viertel der Berechnungen ausgeführt aber trotzdem konnte kein Thread-Block schneller seine Arbeit verrichten, wodurch die Ausführungszeit des Kernels gleich blieb.

Dies motivierte das nächste Arbeitspaket.
Das Umsetzen des oben genannten Ziels, indem nur so viele Work-Items, wie benötigt, gestartet werden und das Abbilden dieser Work-Items auf die entsprechenden Bildpunkte.
Mit der Motivation, dass so wenige Work-Items wie möglich innerhalb einer Work-Group auf die Ausführung anderer Work-Items warten müssen und die Thread-Blöcke möglichst konvergieren.
Zusätzlich gehörte dazu, dass ein weiterer Kernel geschrieben wird, der die anschließende Interpolation der Bildpunkte ausführt.

Die Integration von diesem Arbeitspaket erfordert die Berechnung der mindestens benötigten Anzahl an Work-Items, das Unterteilen in einen x- und y-Wert für die Dimensionen der Work-Items, zur Ausführung durch den Kernel.
Anschließend muss der Raycast-Kernel mit der zuvor berechneten Anzahl an Work-Items in x- und y-Richtung gestartet werden sowie danach der Kernel für die Interpolation.
Der Interpolation Kernel wird aber mit den Dimensionen des zu berechnenden Bildes für die Dimension der Work-Items gestartet. 

\subsection{Auswahl des Eyetrackers}
Da nun drei verschiedene Ansätze für die Implementierung des wahrnehmungsorientierten Raycast existieren, war es an der Zeit, die Mausposition mit der richtigen Blickposition zu ersetzen.
Dies war die Grundlage für das nächste Arbeitspaket.
Dieses umfasste die Auswahl eines geeigneten Eyetrackers aus den verfügbaren, sowie die Einbindung der Programmierschnittstelle des Eyetrackers in das Projekt, um die Eyetrackingdaten verwenden zu können und schließlich die Mausposition mit dem zuletzt gemessenen Blickpunkt zu ersetzen.

Die Auswahl lag in diesem Fall zwischen einer Eyetracking-Brille und dem an einem Monitor angebrachten Tobii Pro Spectrum Eyetracker.
Da die Eyetracking-Brille eine deutlich geringere Abtastrate und Präzision als der Tobii Pro Spectrum, welcher mit bis zu 1200\,Hz und hoher Qualität Blickbewegungen erfassen kann, fiel die Entscheidung hier auf den monitorbasierten Eyetracker Tobii Pro Spectrum.
Die Tobii Pro SDK erlaubte eine einfache Integration des Eyetrackers in das Projekt.
Die Daten des Eyetrackers wurden dem Volumerenderwidget über eine Callback-Funktion zur Verfügung gestellt, welches immer die zuletzt erhaltene gültige Blickposition für die Berechnung des nächsten Bildes verwendet hat.
Die Blickposition wird von dem Eyetracker normiert in $[0,1]^2$ angegeben und musste daher zuvor in die Bildkoordinaten des Volumerenderwidgets umgerechnet werden.
Nun kann aktiviert werden, dass Eyetracking verwendet wird.
Ist dies aktiviert, wird bei der Berechnung eines Bildes statt der letzten Mausposition nun die letzte Blickposition verwendet.

\subsection{Testen der Implementierungen und verschiedener Parameter}
Zu diesem Zeitpunkt existierte der standardmäßige Raycast und zwei verschiedene Variationen, die aus vorhergegangen Arbeitspaketen entstanden sind.
Die erste Variation entstand aus dem Arbeitspaket im Teilabschnitt \ref{ss::MDC}, wobei das Bild einmal mit einem viertel der Auflösung und einmal nur in einem variablen rechteckigen Bereich um den Mauszeiger herum in normaler Auflösung berechnet wurde.
Die beiden Bilder wurden anschließend zu einem Bild zusammengefügt.
Die zweite Variation entstand aus dem Arbeitspaket im Teilabschnitt \ref{ss::DDC}.
Hier wird der Raycast Kernel nur einmal gestartet und sich von der Idee, dass die ID eines Work-Items der Bildposition seines Ergebnisses entspricht, komplett gelöst.
Die IDs der Work-Items werden auf verschiedene Bildpunkte abgebildet und das Bild besteht letztendlich aus drei Bereichen.
Der äußerste Bereich hat die schlechteste Auflösung und umrahmt die inneren Bereiche.
Die inneren Bereiche haben die Form von Ellipsen und sind um den Mauszeiger als ihren Mittelpunkt herum positioniert.
Der innerste Bereich ist dabei kleiner als der mittlere und hat die normale Auflösung.
Der mittlere Bereich hat eine eine Auflösung, die zwischen dem innersten und dem äußersten Bereich liegt, so dass die gesamte Auflösung des Bildes von der Blickposition her nach außen hin in zwei Stufen abnimmt.

Das nächste Arbeitspaket bestand nun daraus, mit der Implementierung aus Teilabschnitt \ref{ss::MDC}, abgekürzt mit \emph{MDC}, und der Implementierung aus Teilabschnitt \ref{ss::DDC}, abgekürzt mit \emph{DDC}, verschiedene Renderingparameter und Transferfunktionen zu testen.
Für die erste Variation (\emph{MDC}) kann dabei die Größe des Rechtecks verändert werden.
Für die zweite Variation (\emph{DDC}) kann die Auflösung der verschiedenen Bereiche sowie jeweils die Größe der inneren Ellipse, die den innersten Bereichs begrenzt und der äußeren Ellipse, die den mittleren Bereich begrenzt, angepasst werden.

\todo{Sollen die verschiedenen Parameter hier schon angegeben werden oder erst im Abschnitt \ref{chap::results} bei den Ergebnissen?}

\subsection{Erstellen von Messwerten}

% Idee: Messung von Kernelzeiten um verschiedene Verfahren besser vergleichen zu können und Messung der gesamten Ausführung um Gefühl für den Overhead der Kernel-Enqueues und der paintGL() Methode zu erhalten und die "echte" Performanz, die der Nutzer erfährt, zu bestimmen.
% -> Arbeitspaket: Integrieren von Möglichkeiten um Messwerte zu nehmen.
% -> Kernelzeiten durch volumerendercl-Objekt
% -> paintGL() Ausführungszeit mit QTime-Objekt
% -> Erstellen einer Mausbewegung über den Bildschirm. Dafür Unterteilung in 10x10 Grid
% -> Einrichten der Möglichkeit, diese zu reproduzieren, um die gleiche Mausbewegung mit den drei verschiedenen Verfahren zu testen (Standard, DDC, MDC)
% -> Erstellen von Messwerten für verschiedene Volumendaten, Transferfunktionen und Renderoptionen

\subsection{Darstellen der Messwerte}
% Darstellung von Mausposition auf dem Bild und der gemessenenen Ausführungszeit.
% Kernelzeiten sind am wichtigsten, da diese die eigentliche Ausführungszeit der unterschiedlichen Methoden repräsentieren.
% Overhead muss nicht extra dargestellt werden, da relativ Konstant.
% -> Arbeitspaket: Darstellen der Messwerte durch Heatmaps
% -> Messen der Auslastung der GPU, wie Warps und Speicherbanks, mit CodeXL (gab aber Probleme und hat schließlich nicht funktioniert.)

\todo{Vorüberlegungen zur Umsetzung und daraus entstandene Arbeitspakete beschreiben.}