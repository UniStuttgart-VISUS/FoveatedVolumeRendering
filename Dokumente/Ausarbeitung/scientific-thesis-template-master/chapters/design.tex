% Entwurf: Projekt, Raytracer, (Auswahl des) Eyetracker, Vorüberlegungen zur Umsetzung / Integration, Grundidee zur Umsetzung
% Entwurfsziele an die Umsetzungs
\chapter{Entwurf}\label{chap::design}
Die Implementierung dieser Arbeit beruht auf einem Visual Studio Projekt zum Volumen Rendering von Valentin Bruder.
Der erste Abschnitt des Entwurfskapitels beinhaltet den Ausgangspunkt der Implementierung dieser Arbeit in Form einer Beschreibung des ursprünglichen Projekts.
Dies umfasst die allgemeine Architektur der Anwendung und die Umsetzung des Volumenrenderings durch eine Implementierung eines Raycasters als OpenCL Kernel.
Der zweite Abschnitt des Kapitels umfasst Überlegungen für die Erweiterung des Projektes, bezüglich des Ausgangspunktes wie er im ersten Abschnitt beschrieben wurde und dem Ziel dieser Arbeit, sowie die daraus entstandenen Arbeitspakete und die Ansätze für ihre Integration in das bestehende Projekt.

\section{Projekt}\label{sec::proj}
% Qt
Das Visual Studio Projekt, welches als Ausgangspunkt dient, ist eine auf Qt basierende Anwendung.
Qt ist ein vollständiges Cross-Platform Softaware Development Framework, welches in c++ entwickelt wurde, und ermöglicht die einfache Erstellung von Anwendungen mit Benutzeroberflächen und bietet außerdem eine Vielzahl von Bibliotheken, die für eine leichtere und schnellere Entwicklung von Programmen genutzt werden können.

% Struktur von Qt
Die Hauptelemente für die Qt Benutzeroberfläche sind Qt Widgets.
Widgets können Daten darstellen und Nutzereingaben erkennen.
Außerdem stellt ein Widget selbst ein Container für weitere Widgets dar, welche in diesem gruppiert werden.
Innerhalb eines Widgets können Elemente platziert werden, welche Informationen, mögliche Operationen oder Nutzereingaben repräsentieren.
Für die bequeme Erstellung der grafischen Oberfläche bietet Qt die Anwendung Qt Designer.
Der Qt Designer ermöglicht es, Widgets und andere Bausteine der grafischen Oberfläche per Drag und Drop anzuordnen.
Die durch den Qt Designer definierte Oberfläche kann abgespeichert werden und wird von Qt verwendet, um eine c++ Header File zu erstellen.
Dies ermöglicht es die verschiedenen Elemente der grafischen Oberfläche an die Logik der Anwendung zu binden. \cite{Qt}

% Struktur des Projekts (verschiedene Widgets)
Das Projekt ist dementsprechend in c++ geschrieben und die grafische Oberfläche wurde mit Hilfe des Qt Designers erstellt.
Die Oberfläche selbst hat eine Menüleiste, die es unter anderem ermöglicht, Volumendaten oder Transferfunktionen zu laden oder auch aktive Transferfunktionen zu speichern sowie das Erstellen eines Screenshots des zuletzt berechneten Bildes.
Den Großteil der grafischen Oberfläche wird durch das Volumenrenderwidget ausgefüllt.
Das Volumenrenderwidget ist ein Qt OpenGL Widget und kann für das Darstellen von OpenGL Grafiken verwendet werden.
Die berechneten Grafiken werden mit Hilfe des Volumenrenderwidgets dargestellt.
Neben dem Volumenrenderwidget gibt es noch ein Widget, welches in drei weitere Widgets unterteilt ist, mit denen Parameter für das Volumenrendering gesetzt werden können.
Das erste von ihnen ermöglicht das variieren der Abtastrate im Bildraum, also die Anzahl der Strahlen, die ausgesendet werden, sowie das Setzen der allgemeinen Abtastrate der ausgesendeten Strahlen.
Außerdem können hier weitere Rendering Parameter festgelegt werden, wie die Hintergrundfarbe oder ob Voxel beim Abtasten interpoliert werden.
Das zweite Widget ist ein Farbenrad, welches für die einfache Auswahl der Farben einzelner Kontrollpunkte der Transferfunktion verwendet werden kann.
Das dritte Widget ermöglicht schließlich das Setzen von Kontrollpunkten der Transferfunktion innerhalb eines Diagramms.
Die x-Richtung gibt die Dichte an, auf die sich ein Kontrollpunkt bezieht.
Die y-Richtung gibt seinen Opazitätswert an.
Die Werte zwischen zwei Kontrollpunkten werden entweder linear oder quadratisch interpoliert.
Daher gibt es immer mindestens einen Kontrollpunkt für den Dichtewert null und einen Kontrollpunkt für den Dichtewert eins.

% Speziell VolumeRenderWidget: OpenCL, OpenGL Host Code
Das Volumenrenderwidget ist ein OpenGL Widget und für die Darstellung der berechneten Bilder zuständig.
Das Qt Framework erlaubt es, das Widget in c++ Code mit Logik zu verknüpfen.
Dafür existiert in dem Projekt eine Klasse VolumeRenderWidget, welche von QOpenGLWidget erbt.
Die grundsätzliche Funktionalität zum Rendern in diesem Widget wird durch die Methode \texttt{paintGL()} ausgeführt.
Innerhalb dieser Methode wird der Code geschrieben, der für die Darstellung des Bildes nötig ist.
Das Darstellen auf dem Bildschirm beziehungsweise in dem Widget wird durch OpenGL realisiert.
OpenGL zeichnet dabei aber lediglich eine durch einen OpenCL Kernel zuvor generierte Textur auf ein Fullscreen Quad.
Das Management von OpenCL wird hier durch ein Objekt der Klasse \texttt{volumerendercl} geregelt.
Innerhalb der \texttt{paintGL()} Methode wird eine Methode dieses Objekts zum Starten des OpenCL Kernels für den Raycast des Volumenrenderings aufgerufen.
Das \texttt{volumerendercl} Objekt regelt die Handhabung der verschiedenen Parameter für den Kernel und die Unterteilung der übergebenen Anzahl an Strahlen in x- und y-Richtung, welche der Anzahl zu startenden Work-Items entspricht, in Work-Groups.
Außerdem startet es den Kernel, synchronisiert einen gemeinsamen Stopp und speichert die benötigte Zeit der letzten Ausführung des Kernels.
Die berechneten Werte der einzelnen Work-Items können bei der Ausführung des Kernels direkt in die OpenGL Textur geschrieben werden.
Daher kann nach der Ausführung der aufgerufenen Methode des \texttt{volumerendercl} Objekts die Textur direkt gezeichnet werden.
Mit Hilfe von Qt Funktionen und der Information über die Ausführungszeit des Kernels werden anschließend noch ein paar Overlays gezeichnet.
Unter Anderem eine Anzeige der ungefähren möglichen Anzahl an Bildern pro Sekunde der letzten Ausführungen, um die Ausführungsdauer des Kernels abschätzen zu können.

% Raycaster: Raycastkernel
\subsection*{Raycaster}\label{ss::rc}
% Raycaster: wichtige Parameter: in, out; Ungefährerer Aufbau: Sampling Loop durch das Volumen;
Der eigentliche Raycast passiert in einem OpenCL Kernel.
Die OpenCL Objekte werden von dem \texttt{volumerendercl} Objekt gehandhabt, dessen Methoden innerhalb der \texttt{paintGL()} Methode aufgerufen werden.
Das \texttt{volumerendercl} Objekt regelt auch die Übergabe der Parameter an den Raycast Kernel.
Dies sind Parameter wie Volumendaten, Transferfunktionswerte und Strahlabtastrate zum lesen, sowie eine 2D-Textur zum schreiben für die berechneten Farbwerte der einzelnen Work-Items.
Jedes Work-Item besitzt eine 2D-ID, die einer Position in der Ausgabetextur zugewiesen bekommt.
Ein Work-Item ist für das Abtasten eines Strahls verantwortlich.
Ein Strahl hat als Ursprung die Position der Kamera und entsprechend seiner ID, beziehungsweise Texturkoordinaten, wird seine Richtung bestimmt.
Abhängig von der Abtastrate des Strahls, berechnet sich die Schrittgröße für das Abtasten. 
Ausgehend von dem Schnittpunkt des Strahls mit der Bildebene wird nun in einer Schleife der Strahl schrittweise abgetastet.
Dabei wird für jeden Schritt die aktuelle Position bestimmt, welche normiert und dann dafür genutzt wird, um in dem 3D-Volumen Objekt den Dichtewert für diese Position zu bestimmen.
Mit dem Dichtewert und den Daten der Transferfunktion wird anschließend ein Farbwert berechnet.
Dieser Farbwert wird mit den bisherigen gesammelten Farbwerten des Strahls verrechnet, so dass am Ende der Abtastschleife ein einziger Farbwert für den Strahl existiert.
Der Farbwert wird zum Schluss an die entsprechende Texturkoordinate in der Ausgabetextur gespeichert.

%  Raycaster: Spezielle Eigenschaften, die aktiviert und deaktiviert werden können: ESS, Interpolation, AO
Der Raycast Kernel hat außer der grundlegenden Raycast Funktion noch weitere Eigenschaften, die die Performanz der Ausführung und die Qualität des Bildes verbessern.
So kann \emph{Empty Space Skipping} aktiviert werden, um größere Bereiche mit rein transparenten Voxeln zu überspringen.
Dies wird mit Hilfe eines zuvor gröber Berechneten Volumen ermöglicht.
Da das Volumen nur eine begrenzte Auflösung hat aber an einer beliebigen Position ein Wert aus dieser 3D-Textur abgerufen werden kann, muss angegeben werden, wie dieser Wert abhängig von den umliegenden Voxel bestimmt wird.
Daher kann hier gewählt werden, dass beim Auslesen des Dichtewerts an einer bestimmten Position des Volumens, dieser interpoliert wird.
Außerdem kann eine Orthografische Sicht des Volumens aktiviert werden, indem die Strahlen parallel ausgesendet werden und für solide Oberflächen gibt es die Möglichkeit, den Effekt der \emph{Ambient Occlusion} darzustellen.

\section{Arbeitspakete und Integration}\label{sec::workpacks}
Ausgehend von der in Abschnitt \ref{sec::proj} beschriebenen Ausgangslage des Projekts, wurden einige Vorüberlegungen und Arbeitspakete erstellt, welche das Ziel hatten, das Projekt so zu erweitern, dass die Aspekte des wahrnehmungsorientierten Volumenrendering veranschaulicht und umgesetzt werden können.
Im folgenden werden die für dieses Ziel entstandenen Vorüberlegungen und daraus erstellte Arbeitspakete aufgeführt und die dazugehörigen Ansätze zur Integration in das bestehende Projekt skizziert.
Genauere Angaben zur Implementierung bestimmter Arbeitspakete werden im Kapitel \ref{chap::impl} vorgestellt.

\subsection{Einarbeitung in das Projekt}\label{sec::workpacks::eidp}
Das erste Arbeitspaket, welches nicht zu vernachlässigen ist, ist die Einarbeitung in das Projekt beziehungsweise in die bestehende Implementierung.
Dies erfordert das Einarbeiten in einige Grundlagen der c++ Programmierung sowie in den grundlegenden Umgang mit dem Qt Framework.
Da das Projekt ein Visual Studio Projekt ist und Programmierschnittstellen wie OpenCL oder auch Qt verwendet, ist das Erlangen einer Kenntnisse für das richtige Verlinken der Bibliotheken mit dem Projekt auch Teil dieses Arbeitspakets.

\subsection{Simulieren der Blickposition}\label{sec::workpacks::sdb}
Um die Eigenschaften des visuellen Wahrnehmungssystems des Menschen auszunutzen, dass die Genauigkeit des Auges außerhalb des zentralen Bereichs stark abnimmt, ist es notwendig, die Blickposition beziehungsweise den fokussierten Punkt auf dem Bildschirm zu kennen.
Ein Eyetracker kann dies messen und die Daten der Anwendung zur Verfügung stellen.
Die Einbindung eines Eyetrackers ist für die ersten Arbeitsschritte, wie das Implementieren erster Versuche und den Raycast Kernel wahrnehmungsorientiert umzuschreiben, nicht notwendig und kann dies sogar behindern.
Das Projekt bietet aber einfache Möglichkeiten auf Maus- oder Tastatureingaben zu reagieren.
Daher ist das zweite Arbeitspaket, das Erkennen von Mausbewegungen innerhalb des Volumerenderwidgets und das Abspeichern der letzten erkannten Mausposition in einer globalen Variable, um die Blickposition mit der Maus simulieren zu können.

\subsection{Reduzierung der Strahlabtastrate im peripheren Bereich}\label{sec::workpacks::ors}
Nun ist es möglich, die Mausposition zu verwenden, um den Blickpunkt auf dem Bildschirm zu simulieren.
Dies erlaubt das Erstellen und Testen von wahrnehmungsorientierten Implementierung. 
Das ursprüngliche Projekt stellt zwei Parameter zur Verfügung, welche auf zwei verschiedene Arten die Ausführungszeit des Kernels beeinflussen.
Die Abtastrate der jeweiligen Strahlen und die Anzahl der Strahlen in x- und y-Richtung.
Da das Verändern der Anzahl an Strahlen in x- und y- Richtung das gesamte Bild betrifft und dies nicht einfach abhängig von dem Abstand zur Mausposition verändert werden kann, ist die Anpassung der Abtastrate einzelner Strahlen für den Anfang einfacher zu gestalten.

Das dritte Arbeitspaket umfasst die Verwendung der an den Raycast Kernel übergebenen Mausposition, um die Abtastrate der jeweiligen Strahlen, abhängig von der Distanz der Bildposition des Strahls zu der Position des Mauszeigers, zu reduzieren.
Die Anpassung des Kernels hat zur Folge, dass für jeden Strahl abhängig seiner Distanz zum Mauszeiger eine eigene Abtastrate berechnet wird.
Aufgrund dessen, dass wie im Abschnitt \ref{ss::rc} die Work-Items den Texel zugeordnet sind und die Work-Groups quadratisch angeordnet sind, soll dies bewirken, dass die Work-Items innerhalb der selben Work-Group eine ähnliche Abtastrate für ihren Strahl berechnen und die gesamte Work-Group früher terminieren kann.
Da der Kernel erst beendet wird, wenn alle Work-Groups ihre Arbeit abgeschlossen haben, würde eine schnellere Ausführung einzelner Work-Groups eine insgesamt schnellere Ausführung des Kernels und auch eine bessere Performanz beim Berechnen des Bildes bewirken.

\subsection{Reduzierung der Strahldichte im peripheren Bereich}\label{ss::MDC}
Trotz der Reduzierung der Abtastrate der Strahlen, wird weiterhin die gleiche Anzahl an Work-Items gestartet.
Dies ermöglicht eine weitere Möglichkeit, die Ausführungszeit des Kernels zu reduzieren, indem die Anzahl der Strahlen und somit auch die Auflösung des berechneten Bildes variiert wird.
Weniger zu berechnenden Strahlen bedeutet hier auch weniger benötigte Work-Items und Work-Groups, die ausgeführt werden müssen.
Weniger Work-Groups bedeutet dann auch eine geringere Ausführungszeit des Raycast Kernels.

Die erste Überlegung diesbezüglich war es, eine Art virtuelle Linse vor die Bildebene zu setzen, die die Strahlen so auf der Bildebene verteilt, dass an der Mausposition eine höhere Strahldichte existiert und diese mit größerem Abstand zur Mausposition abnimmt.
So könnte bei einer geringeren Anzahl an Strahlen, an der Mausposition, die den Blickpunkt simuliert, trotzdem die maximale Auflösung erreicht werden.
Die Bildpunkte die dann nicht direkt von einem Strahl abgedeckt werden, müssten interpoliert werden.
Dieser Ansatz wurde aber verworfen, da die Implementierung der virtuellen Linse und der anschließenden Interpolation sich als zu aufwändig erwies und es deutlich einfacher umzusetzende Alternativen gibt.

Eine Alternative ist es, zwei Mal das selbe Bild, mit jeweils unterschiedlichen Auflösungen, also einer unterschiedlichen Anzahl an Work-Items, zu berechnen.
Daher umfasst das nächste Arbeitspaket das Berechnen des Bildes in zwei verschiedenen Auflösungen und die anschließende Zusammenfügung beider Bilder zu einem.
Dabei soll der normal aufgelöste Bereich an der Blickposition sein und der restliche Teil des Bildes hat die niedrigere Auflösung.
Wie oben beschrieben soll eine Reduzierung der Auflösung auch die Anzahl der gestarteten Work-Items und Work-Groups reduzieren.
Dadurch soll das Bild mit einer geringeren Auflösung deutlich schneller berechnet werden können.
Trotzdem muss ein Teil des Bildes in normaler Auflösung berechnet werden.
Da aber nur ein kleiner Teil in dieser Auflösung berechnet werden muss, soll durch das frühzeitige Abbrechen von Work-Items, die außerhalb dieses Bereichs liegen, die zweite Bildberechnung ebenfalls beschleunigen.
Das Ziel ist, dass die Berechnung zweier Bilder in verschiedenen Auflösungen und das anschließende Interpolieren sowie Zusammenfügen ihrer, eine insgesamt niedrigere Berechnungszeit benötigt, als die Berechnung eines Bildes in normaler Auflösung.
Zusätzlich soll dadurch, dass ein kleiner Bereich des Ergebnisbildes an der Blickposition, der leicht größer als die projizierte Fovea ist, die normale Auflösung hat und die Auflösung im peripheren Bereich trotzdem ausreichend ist, die Wahrnehmung geschaffen werden, dass das gesamte Bild mit normaler Auflösung berechnet wurde.

\subsection{Index-Mapping}\label{ss::DDC}
Das Ziel eines weiteren Ansatzes ist es, die Auflösung von der Mausposition weg, noch weiter zu reduzieren.
Zwischen dem Bereich, der in einer normalen Auflösung und dem Bereich, der in einer niedrigen Auflösung berechnet wird, soll dafür ein weiterer Bereich eingefügt werden, um eine starke Differenz der Bildauflösung an aneinandergrenzenden Bereichen zu verhindern.

Das berechnete Bild besteht dann aus drei Bereichen.
Ein äußerer Bereich und zwei innere Bereiche.
Die inneren Bereiche sollen die Form von Ellipsen haben, werden also durch Ellipsen beschränkt.
Der Mittelpunkt der beiden Ellipsen soll dem Blickpunkt entsprechen, so dass die Auflösung in der Fovea am besten ist und nach außen hin abnimmt.
Die drei Bereiche werden jeweils interpoliert und ergeben dann das gesamte Bild.

Dies motivierte das nächste Arbeitspaket.
Das Berechnen des Bildes mit einer sehr niedrigen Hintergrundauflösung und einer mittleren und normalen Auflösung innerhalb einer mittelgroßen und kleinen Ellipse um dem Blickpunkt.
Die Implementierung sollte diesmal aber nicht aus der Berechnung von drei zusammengefügter Bilder bestehen, sondern aus einer Berechnung eines unterschiedlich stark aufgelösten Bildes und der anschließenden Interpolation der einzelnen Bereiche.
Um dies umzusetzen wird sich von der Annahme, dass die ID eines Work-Items die Bildposition des zugehörigen Strahls ist, vollständig getrennt.
Hier werden nun die IDs der Work-Items auf von ihrer ID her sehr unterschiedlichen Bildpunkte abgebildet.

\subsection{Auswahl des Eyetrackers}
Für das Ersetzen der Mausposition durch Blickpositionen auf dem Bildschirm, ist es notwendig, diese zu messen und die Daten der Anwendung zur Verfügung zu stellen.
Dies ist die Grundlage des nächsten Arbeitspakets.
Es umfasst die Auswahl eines geeigneten Eyetrackers, sowie die Einbindung der Programmierschnittstelle des Eyetrackers in das Projekt.
Anschließend sollen die erhaltenen Eyetrackingdaten in Blickpositionen innerhalb des Volumerenderwidgets umgerechnet werden, um schließlich die Mausposition mit dem zuletzt gemessenen Blickpunkt zu ersetzen zu können.

Die Auswahl lag in diesem Fall zwischen einer Eyetracking-Brille und dem an einem Monitor angebrachten Tobii Pro Spectrum Eyetracker.
Da die Eyetracking-Brille eine deutlich geringere Abtastrate und Präzision als der Tobii Pro Spectrum, welcher mit bis zu 1200\,Hz und hoher Qualität Blickbewegungen erfassen kann, fiel die Entscheidung hier auf den monitorbasierten Eyetracker Tobii Pro Spectrum.
Die Tobii Pro SDK erlaubte eine einfache Integration des Eyetrackers in das Projekt.
Die Daten des Eyetrackers wurden dem Volumerenderwidget über eine Callback-Funktion zur Verfügung gestellt, welche immer die zuletzt erhaltene Blickposition für die Berechnung des nächsten Bildes liefert.
Bevor diese in einer globalen Variable abgespeichert wurde, wurde sie anhand der Validity-Codes auf ihre Gültigkeit überprüft.
Die Blickposition wird von dem Eyetracker normiert in $[0,1]^2$ angegeben und musste daher zuvor in die Bildkoordinaten des Volumerenderwidgets umgerechnet werden.
Wird nun der Eyetracker verwendet, wird bei der Berechnung eines Bildes statt der letzten Mausposition die letzte Blickposition verwendet.

\subsection{Testen der Implementierungen und verschiedener Parameter}
Nachdem die zuvor genannten Arbeitspakete umgesetzt wurden, existierten drei verschiedene Möglichkeiten, den Raycast für das Volumenrendering durchzuführen.
Der standardmäßige Raycast und zwei Variationen, die aus vorhergegangen Arbeitspaketen entstanden sind.
Die erste Variation entstand aus dem Arbeitspaket im Teilabschnitt \ref{ss::MDC}, wobei das Bild einmal mit einem viertel der Auflösung und einmal nur in einem variablen rechteckigen Bereich um den Mauszeiger herum in normaler Auflösung berechnet wurde.
Die beiden Bilder wurden anschließend zu einem Bild zusammengefügt.
Die zweite Variation entstand aus dem Arbeitspaket im Teilabschnitt \ref{ss::DDC}.
Hier wird der Raycast Kernel nur einmal gestartet und sich von der Idee, dass die ID eines Work-Items der Bildposition seines Ergebnisses entspricht, komplett gelöst.
Die IDs der Work-Items werden auf verschiedene Bildpunkte abgebildet und das Bild besteht letztendlich aus drei Bereichen.
Der äußerste Bereich hat die schlechteste Auflösung und umrahmt die inneren Bereiche.
Die inneren Bereiche haben die Form von Ellipsen und sind um den Mauszeiger als ihren Mittelpunkt herum positioniert.
Der innerste Bereich ist dabei kleiner als der mittlere und hat die normale Auflösung.
Der mittlere Bereich hat eine Auflösung, die zwischen dem innersten und dem äußersten Bereich liegt, so dass die gesamte Auflösung des Bildes von der Blickposition her nach außen hin in zwei Stufen abnimmt.

Das nächste Arbeitspaket bestand aus dem Testen der Implementierungen.
Dabei sollte mit der Implementierung aus Teilabschnitt \ref{ss::MDC}, abgekürzt mit \emph{MDC}, und der Implementierung aus Teilabschnitt \ref{ss::DDC}, abgekürzt mit \emph{DDC}, verschiedene Renderingparameter und Transferfunktionen getestet werden.
Die erste Variation (\emph{MDC}) erlaubte es dafür, die Größe des normal aufgelösten Bereiches, welcher die Form eines Quadrates hat, zu verändern.
Bei der zweiten Variation (\emph{DDC}), kann die Auflösung der verschiedenen Bereiche sowie jeweils die Größe der inneren Ellipse, die den innersten Bereichs begrenzt und der äußeren Ellipse, die den mittleren Bereich begrenzt, angepasst werden.

\subsection{Erstellen von Messwerten}\label{sec::workpacks::evm}
Mit den Implementierungen \emph{MDC} und \emph{DDC} sowie ihren bestimmten Parametern ist nun das Ziel, ihre Performanz möglichst reproduzierbar zu messen. Da die Veränderungen selbst hauptsächlich im Kernel existierten, sollten die Ausführungszeiten des Kernels von \emph{MDC} und \emph{DDC} gemessen werden, um die beiden Verfahren untereinander und mit der Standardimplementierung besser vergleichen zu können.
Trotzdem beeinflusst der Overhead der Verfahren bei der Ausführung der \texttt{paintGL()} Methode die reale Performanz, daher sollte diese auch gemessen werden.

Dies ist der Hintergrund für das nächste Arbeitspaket, welches die Integration von Möglichkeiten, um reproduzierbare Messwerte der unterschiedlichen Verfahren \emph{MDC}, \emph{DDC} und des Standard Raycasts zu erstellen, beinhaltet.

Für die Erstellung der Messwerte wurde eine Mausbewegung über das aktuell angezeigte Bild aufgenommen und diese für die unterschiedlichen Varianten mit jeweils unterschiedlichen Volumen und Transferfunktionen wieder abgespielt.
Dabei wurde für jede aufgenommene Mausposition eine Messung vorgenommen und gespeichert.
Um nicht unnötig viele Messwerte zu erstellen, wurde das Bild in $10\times10$ Felder unterteilt und immer nur dann eine Mausposition für die Messung verwendet, falls diese in einem anderen Feld war, als die vorhergegangene.
Die Speicherung aller relevanten Parameter einer Messung, inklusive der Mausbewegungen, soll möglichst reproduzierbare Messungen ermöglichen.

\subsection{Darstellen der Messwerte}
Für die gemessenen Messwerte ist es nun notwendig, diese in eine geeignete Darstellung zu bringen.
Das Ziel war es hier, die Mauspositionen und die jeweils gemessene Ausführungszeit in einem Diagramm darzustellen.
Hier wurde sich für eine Repräsentation mittels einer Heatmap entschieden.
Das letzte Arbeitspaket umfasste daher die Aufgabe, die Messwerte je Volumen, verwendeter Transferfunktion und verwendetem Verfahren für die Bildberechnung, in einer verständlichen Form darzustellen.
Dies wurde in Form von Heatmaps, Boxplots und einer Tabelle umgesetzt.
Für die Heatmap sollte das Bild, welches mit der Standardvariante und einer bestimmten Transferfunktion erstellt wurde, als Hintergrund des Plots dienen.
Die entsprechenden Mausposition sollen über das Bild als Punkte mit bestimmten Farben eingezeichnet werden.
Die Farbe eines Punktes soll angeben, wie lange der Kernel ausgeführt wurde, um das Bild mit der Mausposition an dieser Stelle zu berechnen.
Für ein Volumen und einer bestimmten Transferfunktion soll ein solcher Plot jeweils für die Messwerte von \emph{MDC} und \emph{DDC} erstellt werden.