% Grundlagen für Arbeit (Related Work), Sehapparat, Raytracing, Volumenrendering, Eyetracking, GPU Architektur (Warps usw.) %

\chapter{Grundlagen}\label{chap::basics}
\label{chap:k2}
Dieses Kapitel beschäftigt sich mit den Grundlagen für diese Arbeit.
Der erste Abschnitt des Kapitels beschäftigt sich mit verwandten Arbeiten zum Thema Wahrnehmungsorientiertes Volumen-Rendering.
Der zweite Abschnitt handelt über die Grundlagen des menschlichen Sehapparates.
Hier werden die Fähigkeiten und Limitierungen der visuellen Wahrnehmung des Menschen diskutiert.
In Abschnitt drei, wird die Funktionsweise von Raytracing erläutert, welches ein grundlegender Algorithmus, der für diese Arbeit zugrunde liegender Implementierung ist.
Zusammenhängend mit Raytracing wird im Abschnitt vier, die Verwendung des Raytracers für das Volumenrendering erläutert.
Abschnitt fünf diskutiert die Auswahl des für diese Arbeit zugrunde liegenden Eyetrackers und dessen Verwendung für die Erfassung des fovealen und peripheren Bereichs.
Aus Performanzgründen kann es hilfreich sein für Berechnungen auf einer GPU, die Architektur der GPU zu betrachten und unter Umständen Algorithmen für eine bessere Effizienz anzupassen.
Diese Thematik wird in Abschnitt sechs behandelt.
\todo{Beschreibung der Grundlagen überarbeiten}

\section{Related Work}\label{sec::relwo}
Wahrnehmungsorientiertes Volumenrendering ist kein absolut neues Arbeitsgebiet und ist schon Teil einiger wissenschaftlicher Arbeiten gewesen \todo{Beispiele}.
Da diese Arbeit auf zwei trennbare Aspekte beruht, unterteile ich den Related Work Abschnitt in zwei Teile.
Der erste Abschnitt bezieht sich auf Arbeiten im Bereich des wahrnehmungsorientierten Renderings mit dem Ziel, die Performanz einer Anwendung zu steigern.
Die Ansätze hier beziehen sich meist darauf, dass die Qualität der Darstellung im peripheren Bereich der visuellen Wahrnehmung, gesenkt wird und so für die Berechnung eines Bildes weniger Rechenleistung aufgewendet werden muss.
\todo{Was genau ist mit Performanz gemeint?}
Der zweite Abschnitt bezieht sich auf Arbeiten zu wahrnehmungsorientiertem Volumenrendering, mit dem Ziel, die Qualität der Darstellung zu erhöhen.
Dabei werden vor allem Ansätze zur geschickten Anpassung von Parametern einer Transferfunktion vorgestellt, die dem Betrachter ein insgesamt besseres Verständnis der Volumendaten ermöglichen soll.
\subsection{Performanz- und Wahrnehmungsorientiertes Volumenrendering}
In einem Paper von Marc Levoy und Ross Whitaker \enquote{Gaze-Directed Volume Rendering} \cite{Levoy:1990:GVR:91385.91449} erforschten diese Methoden, wie Eyetracking-Daten in Rendering-Algorithmen eingesetzt werden könnnen.
Das Ziel ihrer Forschung war es, dem Nutzer einen Arbeitsplatz für Echtzeit-Volumen-Rendering, mit der Illusion eines hochauflösendes Bildes über den gesamten Bildschirm, zu präsentieren, welches durch Eyetracking unterstützten Verfahren einen geringeren Berechnungsaufwand als herkömmliche Volumen Renderer hat.
Dafür präsentierten sie eine Implementierung eines Ray-Tracers für Volumen Daten, in welcher mit Hilfe der Eyetracking-Daten der Blickfokus auf dem Bildschirm berechnet wurde und um diese Position herum, die Anzahl der Strahlen und die Samples pro Strahl, abhängig von der Distanz zum Blickfokus, angepasst wurden.
Die Implementierung basiert auf darauf, dass der Detaillgrad der visuellen Wahrnehmung des Menschlichen Auge nur in einem kleinen, zentralen Bereich, der Fovea, am höchsten ist und zu den Rändern des Blickfelds hin, der periphere Bereich, stark abnimmt.
Ausgehend davon, berechneten sie in dem von dem Nutzer fokussierten Bereich, das Bild mit der vollen Auflösung und einer hohen Abtastrate der Strahlen.
In dem restlichen Bereich reduzierten sie die Auflösung des Bildes und abhängig von dem Abstand eines Pixels zum Blickfokus, die Abtastrate eines Strahls.
In ihrer Implementierung verwendeten sie 2D und 3D mip maps, einen Eye Tracker und die Pixel-Planes 5 rendering engine, ein hoch paralleles Raster Display System.
Um die geringere Abtastung in der Peripherie gut nutzen zu können, wurde aus den 3D Volumendaten eine 3D mip map erstellt.
Die 3D mip map wird durch den Ray-Tracing Algorithmus abgetastet und durch trilineare Interpolation eine 2D mip map erstellt.
Die 2D mip map ist Grundlage für die Erstellung des endgültigen Bildes.
In ihren Ergebnissen konnten sie die Rendering Kosten für ein teilweise hochauflösendes Bild im Vergleich zu einem vollständig hochauflösenden Bildes, um bis den Faktor fünf, senken.

In einer Arbeit von Guenter et. Al. \enquote{Foveated 3D Graphics} \cite{foveated-3d-graphics}, wurde ähnlich zu dem Paper von Levoy und Whitaker, der Abfall der visuellen Auflösung des Auges außerhalb des visuellen Zentrums, der Fovea, ausgenutzt, um eine beschleunigte Berechnung des Bildes zu erhalten.
Im Gegensatz zu der davor genannten Arbeit, ist das darunterliegende System hier kein Ray-Tracer für Volumendaten, sondern eine Grafikpipeline für 3D Szenen.
Das Bild wird hier aus drei Teilbilder zusammengesetzt, welche jeweils mit unterschiedlichen Auflösungen berechnet wurden und wie Schichten übereinander gelegt werden.
Die drei Schichten sind: innere Schicht, mittlere Schicht und äußere Schicht.
Die innere Schicht hat ungefähr die Größe des fovealen Bereichs auf dem Bildschirm und wird in der maximalen Auflösung berechnet.
Ihr Mittelpunkt ist der Blickfokus des Betrachters.
Die mittlere Schicht ist ein bisschen größer als die innere Schicht und wird mit einer niedrigeren Auflösung berechnet. 
Sie wird ebenfalls auf den Blickfokus des Betrachters zentriert.
Die äußerste Schicht überdeckt den gesamten Bildbereich und wird in der niedrigsten Auflösung berechnet.
Um die Schichten zusammensetzen zu können, werden die mittlere und äußere Schicht jeweils zur nativen Auflösung des Bildschirms, die gleiche Auflösung wie die innere Schicht, interpoliert.
Scharfe Kanten zwischen den Schichten werden dadurch vermieden, dass diese sich leicht überlappen und spezielle Blend-Masken verwendet werden, um die verschiedenen Schichten glatt übereinander zu blenden.
In ihrer Arbeit nahmen sie sich auch das Problem an, dass das starke Unterabtasten, um bis zu dem Faktor sechs in jede Dimension, in der mittleren und äußeren Schicht zu störenden und sich bewegenden Artefakten führen kann.
Um dies entgegen zu wirken, verwendeten sie drei Antialiasing Techniken: Hardware Multi-Sample Antialiasing (MSAA), \enquote{Temporal Reprojection} und \enquote{whole frame jitter sampling}.
Um die Blickposition zu erfassen nutzten sie den Tobii Tx 300 Eye Tracker mit einer Worst-Case Latency von 10 ms.
Das Bild wurde auf einem Computer mit Intel Xeon CPU (E5640 mit 2.67 Ghz) und einer NVidia GeForce GTX 580 GPU berechnet.
Dargestellt wurde es auf einem 23" 1920x1080 LCD Monitor mit einer Bildwiederholungsrate von 120 Hz.
Mit ihrem System und Techniken erlangten sie eine Performanz-Verbesserung von einem Faktor zwischen fünf und sechs auf einem Monitor mit HD Auflösung.
Dabei erreichten sie eine Darstellungsqualität, die vergleichbar mit dem Rendern eines hochauflösenden Bildes über den gesamten Bildschirm ist.

\subsection{Wahrnehmungsorientiertes Volumenrendering zur Qualitätssteigerung}
R. Englund und T. Ropinski stellten in ihrem Paper \enquote{Quantitative and Qualitative Analysis of the Perception of Semi-Transparent Structure in Direct Volume Rendering} \cite{doi:10.1111/cgf.13320} verschiedene Techniken, zur Verbesserung der Wahrnehmung von komplexen volumetrischen Daten, vor.
In einer Studie mit über 300 Teilnehmern untersuchten sie, wie diese Techniken zur verbesserten Wahrnehmung von Volumen beitragen und verglichen die verschiedenen Ansätze miteinander.
Dabei mussten die Teilnehmer jeweils kleine Aufgaben absolvieren, so dass Rückschlüsse darauf geschlossen werden können, wie sehr eine gewisse Technik dem Teilnehmer bei der Erkennung von Form und Tiefe eines Objekts in einem Volumen beiträgt.
Um eine bessere direkte Erforschung der Volumen-Daten später ermöglichen zu können, wurden Techniken, die automatisch Rendering-Parameter angepasst haben hier ausgelassen.
Nur wenn der Nutzer selbst interaktiv sein kann, also die Parameter des Volumen-Renderings, wie die Transferfunktion und Kamera selbst anpassen kann, ermöglicht dies eine direkte Erforschung.
In ihrer Studie haben sie sechs Techniken ausgewertet. Darunter \enquote{Direct Volume Raytracing} (DVR) als grundlegende Technik und \enquote{Depth Darkening} wobei Tiefeneffekte ähnlich zu \enquote{Ambient Occlusion} dadurch hervorgerufen werden, dass tiefere Objekte dunkler gezeichnet werden. 
In ihrer Auswertung kamen sie unter Anderem dazu, dass Techniken, die natürliche Lichteffekte in den Volumendaten simulieren, deutliche Vorteile gegenüber die anderen getesteten Ansätze gezeigt haben.

Anders als zu den untersuchten Techniken von Englund und Ropinski \cite{doi:10.1111/cgf.13320} präsentieren Aidong Lu et. Al. in ihrem Paper \enquote{Volume Composition Using Eye Tracking Data} \cite{Lu:2006:VCU:2384796.2384814} eine Methode für automatisierte Parameterauswahl bei der Betrachtung von Volumen Daten mit Hilfe eines Eye Tracking Gerätes.
Das Ziel, welches sie mit dieser Methode verfolgen ist es, die mühsame Nutzerinteraktionen bei der Auswahl der Parameter zu vereinfachen und damit die Nutzbarkeit des Darstellungssystems zu verbessern.
Volumen Daten können sehr komplex sein und daher ist es oft schwierig herauszufinden, was der Nutzer in dem Volumen betrachten möchte und was dahingehend hervorgehoben werden soll.
Trotzdem ermöglichen Eigenschaften des Volumen Renderings, wie konstante Größen, Formen und Positionen der Objekte eine automatische Anpassung der Parameter.
Um die Bereiche, die für den Nutzer von Interesse sind, zu bestimmen, wird ein Eyetracker zur Hilfe genommen.
Dieser misst die Augenbewegungen und die Blickposition auf dem Bildschirm.
Es wird zwischen zwei Hauptsächlichen Augenbewegungen unterschieden: Sakkade und Fixation.
Eine Sakkade ist eine schnelle Augenbewegung von einem Punkt zu einem anderem.
Bei einer Fixation ruht das Auge auf einem Punkt.
Die Punkte, die fixiert werden, sind meist für den Nutzer von Interesse.
Da die Blickposition durch den Eye Tracker nur auf einer 2D-Ebene bestimmt werden kann, wird durch ein konstantes Rotieren des Volumen Objekts und der parallelen Aufzeichnung der Augenbewegung versucht, die 3D Position des fixierten Objektes zu rekonstruieren.
Aus den Eyetracking und Volumendaten bestimmen sie mit Hilfe mehrerer Clustering-Methoden gewichte für die einzelnen Voxel des Volumens und berechnen so die Wichtigkeit der Objekte innerhalb des Volumens für den Nutzer.
Entsprechend dieser Ergebnisse wurden die Render Parameter angepasst, um die für den Nutzer am interessantesten Objekte hervorzuheben und anzuzeigen.
Aidong Lu et. Al. kamen zu dem Schluss, dass die präsentierte Methode den Aufwand für den Nutzer, die Render Parameter anzupassen, signifikant reduzieren kann.
Trotzdem kann ein solcher regelbasierter Ansatz nicht mit einer manuellen Einstellung der Render-Parameter mithalten.

\todo{Related Work nochmal anschauen. Zum Bsp. ob aus der richtigen Perspektive geschrieben wurde.}

\section{Sehapparat}\label{sec::eye}
Das Auge ist der visuelle Sensor des Menschen und ermöglicht ihm das Sehen.
Wahrnehmungsorientiertes Rendering nutzt gezielt Eigenschaften der visuellen Wahrnehmung des Menschen aus.
Dafür ist es notwendig, ein gutes Verständnis des menschlichen visuellen Wahrnehmungsapparates zu besitzen.
In diesem Abschnitt stelle ich einige Eigenschaften des visuellen Wahrnehmungsapparates vor, wie sie in \cite{doi:10.1111/cgf.13150} vorgeführt werden.

\begin{figure}
	\centering
	\includegraphics[width=0.5\textwidth]{../../Grafiken/HVS-model_from-star-report.png}
	\caption{Modell des visuellen Wahrnehmungsapparates des Menschen aus \cite{doi:10.1111/cfg.13150}}
	\label{fig:eye01}
\end{figure}

\ref{fig:eye01} ist ein Modell des visuellen Wahrnehmungsapparates des Menschen.
Das Modell unterteilt den Sehapparat des Menschen in Optik, Sensorik, Motorik, Verarbeitung, Speicherung und Aufmerksamkeit.
Licht trifft auf die Augen und wird durch die Optik auf die Retina, die Sensorik, weitergeleitet.
Hier wird der visuelle Input abgetastet und gefiltert.
Dabei entstehen zwei Datenströme welche die Verarbeitung stereoskopischer Bilder über einen großen Blickwinkel mit unterschiedlichen räumlich unterschiedlicher Auflösungen ermöglichen.
Die Retina (oder auch Netzhaut) ist mit dem visuellen Kortex verbunden.
Die Signale werden über die visuellen Nerven komprimiert und zum visuellen Kortex transportiert.
Dort werden sie von verschiedenen Bereichen im Gehirn verarbeitet.
Speicherung und Aufmerksamkeit spielen dabei eine wesentliche Rolle.

Das visuelle Wahrnehmungssystem des Menschen hat wesentliche Limitierungen, die bei der Darstellung von Bildern gezielt genutzt werden können.
Die Sehschärfe des ungleich auf der Retina verteilt.
In einem zentralen, kleinem Bereich ist die Sehschärfe maximal.
Dieser Bereich wird Fovea oder auch Gelber Fleck genannt.
Je weiter man sich von der Fovea nach außen hin entfernt, desto mehr nimmt die Sehschärfe ab.
Der Bereich um die Fovea ist der periphere Bereich des Auges.

\todo{Grundlagen zum menschlichen Sehapparat hier.}

\section{Raytracing}\label{sec::rc}
\todo{Grundlagen zu Raytracing hier.}

\section{Volumenrendering und Transferfunktion}\label{sec::voltff}
\todo{Grundlagen zum Volumenrendering und der zugehörigen Transferfunktion hier.}

\section{Eyetracking}\label{sec::eyetr}
\todo{Grundlagen zum Eyetracking hier.}

\section{GPU Architektur}\label{sec::gpuarc}
\todo{Grundlagen zur GPU Architektur hier.}
